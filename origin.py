# -*- coding: utf-8 -*-
"""MLFA25-FC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HRO8v6gv0n8MHOXCGtmkLqv5-MpsuAh
"""

"""
One piece of audio = One piece of image
"""

import os
import random
from pathlib import Path

import librosa
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import f1_score, confusion_matrix
from tqdm import tqdm

# =============================================================================
# Reproducibility
# =============================================================================

def set_seed(seed=42):
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False  # Performance mode
    torch.backends.cudnn.benchmark = True

set_seed(42)

# Device configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BASE_DIR = Path("/scratch/tl3735/MLFA/Kaggle_Data")
TRAIN_CSV = BASE_DIR / "metadata" / "kaggle_train.csv"
TEST_CSV = BASE_DIR / "metadata" / "kaggle_test.csv"
AUDIO_DIR = BASE_DIR / "audio"
TEST_AUDIO_DIR = AUDIO_DIR / "test"

# Number of classes in the dataset
NUM_CLASSES = 10

# Default audio / feature configuration for Phase 1
AUDIO_CONFIG = {
    "sr": 22050,
    "n_mels": 128,
    "n_fft": 2048,
    "hop_length": 512,
    "duration": 4.0,  
    "fmax": 8000,
}

def audio_to_mel_spectrogram(audio_path, sr=22050, n_mels=128, n_fft=2048,
                             hop_length=512, duration=None, fmax=8000):
    """
    Convert audio file to Mel spectrogram (common CNN input format)

    Args:
        audio_path: Path to audio file
        sr: Sample rate (default 22050 Hz)
        n_mels: Number of Mel filter banks (default 128, corresponds to image height)
        n_fft: FFT window size
        hop_length: Hop length for STFT
        duration: Fixed duration in seconds, if specified will truncate or pad

    Returns:
        mel_spec: Mel spectrogram (n_mels, time_frames)
        y: Audio signal
        sr: Sample rate
    """
    # Load audio
    y, original_sr = librosa.load(str(audio_path), sr=sr)

    # Truncate or pad if fixed duration is specified
    if duration is not None:
        target_length = int(sr * duration)
        if len(y) > target_length:
            y = y[:target_length]
        elif len(y) < target_length:
            y = np.pad(y, (0, target_length - len(y)), mode='constant')

    # Compute Mel spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_mels=n_mels,
        n_fft=n_fft,
        hop_length=hop_length,
        fmax=fmax  # Typically focus on 0-8kHz
    )

    # Convert to log scale (Log-Mel Spectrogram, more commonly used)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    return mel_spec_db, y, sr


def normalize_spectrogram(mel_spec):
    """
    Normalize spectrogram to [0, 1] range.
    
    Args:
        mel_spec: np.ndarray, input spectrogram (any shape)
    
    Returns:
        mel_normalized: np.ndarray, normalized to [0, 1], float32
    """
    mel_min = mel_spec.min()
    mel_max = mel_spec.max()
    mel_normalized = (mel_spec - mel_min) / (mel_max - mel_min + 1e-8)
    return mel_normalized.astype(np.float32)


def compute_normalized_mel_spectrogram(audio_path, config=AUDIO_CONFIG, normalize=True):
    """
    High-level helper:
    audio file path -> (optionally normalized) Log-Mel spectrogram

    Args:
        audio_path: Path to audio file
        config: dict with keys like AUDIO_CONFIG
        normalize: Whether to scale to [0, 1]

    Returns:
        mel: np.ndarray of shape (n_mels, time_frames), float32
        y: audio signal
        sr: sample rate
    """
    # Step 1: Convert audio to Log-Mel spectrogram
    mel_spec_db, y, sr = audio_to_mel_spectrogram(
        audio_path=audio_path,
        sr=config.get("sr", 22050),
        n_mels=config.get("n_mels", 128),
        n_fft=config.get("n_fft", 2048),
        hop_length=config.get("hop_length", 512),
        duration=config.get("duration", None),
        fmax=config.get("fmax", 8000),
    )

    # Step 2: Normalize (optional)
    if normalize:
        mel = normalize_spectrogram(mel_spec_db)
    else:
        mel = mel_spec_db.astype(np.float32)

    return mel, y, sr


def process_training_sample(row_index=0, save_npy=True):
    """
    Process a training sample and convert to CNN input format

    Args:
        row_index: Data index
        save_npy: Whether to save as .npy file

    Returns:
        Dictionary containing mel_spectrogram, shape, class_id, class_name, file_name
    """
    print("=" * 60)
    print("Convert audio to CNN input format (Mel Spectrogram)")
    print("=" * 60)

    # Read training data
    df_train = pd.read_csv(TRAIN_CSV)
    row = df_train.iloc[row_index]

    file_name = row['slice_file_name']
    fold = row['fold']
    class_name = row['class']
    class_id = row['classID']

    print(f"\nProcessing sample:")
    print(f"  File name: {file_name}")
    print(f"  Class: {class_name} (ID: {class_id})")
    print(f"  Fold: {fold}")

    # Build audio path
    audio_path = AUDIO_DIR / f"fold{fold}" / file_name

    if not audio_path.exists():
        print(f"Error: File not found: {audio_path}")
        return None

    # Convert to Mel spectrogram
    print(f"\nConverting to Mel spectrogram...")
    print(f"  Parameters: n_mels=128, sr=22050, hop_length=512")

    mel_spec, y, sr = compute_normalized_mel_spectrogram(
        audio_path,
        config=AUDIO_CONFIG,
        normalize=True,
    )

    print(f"\n✓ Conversion complete!")
    print(f"  Original audio length: {len(y) / sr:.2f} seconds")
    print(f"  Sample rate: {sr} Hz")
    print(f"  Mel spectrogram shape: {mel_spec.shape}")
    print(f"    - Height (frequency dimension): {mel_spec.shape[0]} (Mel bins)")
    print(f"    - Width (time dimension): {mel_spec.shape[1]} (time frames)")

    print(f"\n  Normalized range: [{mel_spec.min():.3f}, {mel_spec.max():.3f}]")

    # Save as numpy array (can be directly used for training)
    if save_npy:
        npy_path = f"cnn_input_{row_index}.npy"
        np.save(npy_path, mel_spec)
        print(f"✓ Saved as numpy array: {npy_path}")
        print(f"  File size: {Path(npy_path).stat().st_size / 1024:.2f} KB")
        print(f"  Can be loaded with np.load('{npy_path}') for CNN training")

    return {
        'mel_spectrogram': mel_spec,
        'shape': mel_spec.shape,
        'class_id': class_id,
        'class_name': class_name,
        'file_name': file_name
    }


class UrbanSoundDataset(Dataset):
    """
    PyTorch Dataset that reads kaggle_train.csv and returns
    (Log-Mel spectrogram tensor, classID).
    """

    def __init__(
        self,
        csv_path: Path = TRAIN_CSV,
        audio_dir: Path = AUDIO_DIR,
        folds=None,
        audio_config=None,
        cache_dir: Path | None = None,
    ):
        self.csv_path = Path(csv_path)
        self.audio_dir = Path(audio_dir)
        self.df = pd.read_csv(self.csv_path)

        if folds is not None:
            self.df = self.df[self.df["fold"].isin(folds)].reset_index(drop=True)

        self.audio_config = audio_config or AUDIO_CONFIG
        self.cache_dir = Path(cache_dir) if cache_dir is not None else None
        if self.cache_dir is not None:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

    def __len__(self):
        return len(self.df)

    def _cache_path(self, fold, file_name):
        if self.cache_dir is None:
            return None
        stem = Path(file_name).stem
        return self.cache_dir / f"mel_fold{fold}_{stem}.npy"

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_name = row["slice_file_name"]
        fold = row["fold"]
        class_id = int(row["classID"])

        cache_path = self._cache_path(fold, file_name)
        if cache_path is not None and cache_path.exists():
            mel = np.load(cache_path)
        else:
            audio_path = self.audio_dir / f"fold{fold}" / file_name
            if not audio_path.exists():
                raise FileNotFoundError(f"Audio file not found: {audio_path}")

            mel, _, _ = compute_normalized_mel_spectrogram(
                audio_path,
                config=self.audio_config,
                normalize=True,
            )

            if cache_path is not None:
                np.save(cache_path, mel)

        # (n_mels, time) -> (1, n_mels, time) for CNN input
        mel_tensor = torch.from_numpy(mel).unsqueeze(0)  # [1, 128, T]
        label_tensor = torch.tensor(class_id, dtype=torch.long)
        return mel_tensor, label_tensor


def get_cv_folds(n_folds=8, val_fold=8):
    """
    Generate train/val fold splits for cross-validation.
    
    Args:
        n_folds: Total number of folds (default 8)
        val_fold: Which fold to use as validation (1 to n_folds)
    
    Returns:
        train_folds: tuple of training fold indices
        val_folds: tuple containing the validation fold index
    
    Example:
        >>> get_cv_folds(n_folds=8, val_fold=1)
        ((2, 3, 4, 5, 6, 7, 8), (1,))
        >>> get_cv_folds(n_folds=8, val_fold=8)
        ((1, 2, 3, 4, 5, 6, 7), (8,))
    """
    all_folds = list(range(1, n_folds + 1))
    train_folds = tuple(f for f in all_folds if f != val_fold)
    val_folds = (val_fold,)
    return train_folds, val_folds


def create_train_val_dataloaders(
    train_folds=(1, 2, 3, 4, 5, 6, 7),
    val_folds=(8,),
    batch_size=32,
    num_workers=0,
    cache_dir: Path | None = None,
):
    """
    Convenience function to build train/val DataLoaders.
    
    Note: Data has fold 1-8 only. Use get_cv_folds() for cross-validation.
    
    Example for 8-fold CV:
        for val_fold in range(1, 9):
            train_folds, val_folds = get_cv_folds(n_folds=8, val_fold=val_fold)
            train_loader, val_loader = create_train_val_dataloaders(
                train_folds=train_folds, val_folds=val_folds, ...
            )
            # train model...
    """
    train_dataset = UrbanSoundDataset(
        csv_path=TRAIN_CSV,
        audio_dir=AUDIO_DIR,
        folds=list(train_folds),
        audio_config=AUDIO_CONFIG,
        cache_dir=cache_dir,
    )
    val_dataset = UrbanSoundDataset(
        csv_path=TRAIN_CSV,
        audio_dir=AUDIO_DIR,
        folds=list(val_folds),
        audio_config=AUDIO_CONFIG,
        cache_dir=cache_dir,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
    )

    return train_loader, val_loader


def show_class_distribution(csv_path: Path = TRAIN_CSV):
    """
    Simple EDA: print per-class and per-fold sample counts.
    """
    df = pd.read_csv(csv_path)
    print("\nClass distribution (class -> count):")
    print(df["class"].value_counts())

    print("\nFold distribution (fold -> count):")
    print(df["fold"].value_counts().sort_index())


def get_class_names(csv_path: Path = TRAIN_CSV):
    """
    Get class names sorted by classID.
    
    Args:
        csv_path: Path to the CSV file
    
    Returns:
        list of class names, indexed by classID (0 to NUM_CLASSES-1)
    """
    df = pd.read_csv(csv_path)
    # Get unique class name for each classID
    name_by_id = df.drop_duplicates("classID").set_index("classID")["class"]
    
    class_names = []
    for i in range(NUM_CLASSES):
        if i in name_by_id.index:
            class_names.append(str(name_by_id.loc[i]))
        else:
            class_names.append(str(i))  # Fallback to ID if name not found
    
    return class_names


def compute_class_weights(folds, csv_path: Path = TRAIN_CSV):
    """
    Compute class weights to handle class imbalance.
    
    Formula: weight[c] = total_samples / (num_classes * count[c])
    
    This gives higher weights to minority classes, so the loss function
    penalizes mistakes on rare classes more heavily.
    
    Args:
        folds: tuple/list of fold indices to compute weights for
        csv_path: Path to the CSV file
    
    Returns:
        torch.Tensor of shape (NUM_CLASSES,), dtype=float32
    
    Example:
        >>> weights = compute_class_weights(folds=(1,2,3,4,5,6,7))
        >>> criterion = nn.CrossEntropyLoss(weight=weights.to(device))
    """
    df = pd.read_csv(csv_path)
    
    # Filter by specified folds
    df = df[df["fold"].isin(folds)]
    
    # Count samples per class
    counts = df["classID"].value_counts().sort_index()
    
    # Ensure all classes are present (fill missing with small value to avoid division by zero)
    class_counts = counts.reindex(range(NUM_CLASSES)).fillna(0) + 1e-6
    
    # Compute weights: total / (num_classes * count_per_class)
    total_samples = class_counts.sum()
    weights = total_samples / (NUM_CLASSES * class_counts)
    
    return torch.tensor(weights.values, dtype=torch.float32)


def plot_random_mel_examples(dataset: Dataset, num_samples: int = 3):
    """
    Plot a few random Log-Mel spectrograms from a Dataset for visual EDA.

    This function is meant to be called interactively, e.g. from a notebook.
    """
    import random

    import matplotlib.pyplot as plt
    import librosa.display

    n_samples = min(num_samples, len(dataset))
    indices = random.sample(range(len(dataset)), n_samples)

    for i, idx in enumerate(indices, start=1):
        mel_tensor, label_tensor = dataset[idx]
        mel = mel_tensor.squeeze(0).numpy()  # (n_mels, time)

        plt.figure(figsize=(8, 4))
        librosa.display.specshow(
            mel,
            sr=AUDIO_CONFIG["sr"],
            hop_length=AUDIO_CONFIG["hop_length"],
            x_axis="time",
            y_axis="mel",
        )
        plt.colorbar(format="%+2.0f")
        plt.title(f"Sample {idx} (classID={int(label_tensor)})")
        plt.tight_layout()

    plt.show()


# =============================================================================
# Data Augmentation (from Final-Competition)
# =============================================================================

class SpecAugment:
    """
    SpecAugment: Time and Frequency masking for spectrograms.
    Randomly masks portions of the spectrogram to improve generalization.
    """
    def __init__(self, freq_mask_param=10, time_mask_param=20, 
                 num_freq_masks=1, num_time_masks=1):
        self.freq_mask_param = freq_mask_param
        self.time_mask_param = time_mask_param
        self.num_freq_masks = num_freq_masks
        self.num_time_masks = num_time_masks
    
    def __call__(self, mel_spec):
        """
        Args:
            mel_spec: tensor of shape (1, n_mels, time) or (n_mels, time)
        """
        if mel_spec.dim() == 2:
            mel_spec = mel_spec.unsqueeze(0)
        
        _, n_mels, time_steps = mel_spec.shape
        augmented = mel_spec.clone()
        
        # Frequency masking
        for _ in range(self.num_freq_masks):
            f = random.randint(0, min(self.freq_mask_param, n_mels - 1))
            f0 = random.randint(0, n_mels - f)
            augmented[:, f0:f0 + f, :] = 0
        
        # Time masking
        for _ in range(self.num_time_masks):
            t = random.randint(0, min(self.time_mask_param, time_steps - 1))
            t0 = random.randint(0, time_steps - t)
            augmented[:, :, t0:t0 + t] = 0
        
        return augmented.squeeze(0) if mel_spec.dim() == 2 else augmented


def mixup_data(x, y, alpha=0.2):
    """
    Mixup: Creates virtual training examples by mixing pairs of samples.
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
        lam = max(lam, 1 - lam)  # Ensure dominant sample is preserved
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Loss function for mixup."""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


# =============================================================================
# Model Architecture (from Final-Competition)
# =============================================================================

class BasicBlock(nn.Module):
    """Basic residual block for ResNet."""
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=False):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        if downsample:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.downsample = None

    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        return out


class AudioResNet(nn.Module):
    """
    Custom ResNet for audio classification.
    Based on Final-Competition's high-score solution.
    
    Key modifications for audio:
    - 3x3 initial conv (instead of 7x7) to preserve frequency detail
    - No initial maxpool to retain more information
    - Adaptive pooling for variable-length input
    """
    
    def __init__(self, in_channels=1, num_classes=NUM_CLASSES):
        super(AudioResNet, self).__init__()
        
        # Initial convolution - smaller kernel to preserve frequency detail
        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        # No maxpool - preserve spatial information
        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)
        
        # Residual layers: 64 -> 128 -> 256 -> 512
        self.layer1 = nn.Sequential(
            BasicBlock(64, 64, stride=1, downsample=False),
            BasicBlock(64, 64, stride=1, downsample=False)
        )
        self.layer2 = nn.Sequential(
            BasicBlock(64, 128, stride=2, downsample=True),
            BasicBlock(128, 128, stride=1, downsample=False)
        )
        self.layer3 = nn.Sequential(
            BasicBlock(128, 256, stride=2, downsample=True),
            BasicBlock(256, 256, stride=1, downsample=False)
        )
        self.layer4 = nn.Sequential(
            BasicBlock(256, 512, stride=2, downsample=True),
            BasicBlock(512, 512, stride=1, downsample=False)
        )
        
        # Global pooling and classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(512, num_classes)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # Input: (batch, 1, 128, time)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        return x


# =============================================================================
# Training Functions
# =============================================================================

def train_one_epoch(model, loader, criterion, optimizer, device, use_mixup=False):
    """Train for one epoch with optional mixup."""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(loader, desc="Training")
    for inputs, targets in pbar:
        inputs, targets = inputs.to(device), targets.to(device)
        
        if use_mixup and random.random() > 0.5:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)
            outputs = model(inputs)
            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
            _, predicted = outputs.max(1)
            correct += (lam * predicted.eq(targets_a).sum().item() +
                       (1 - lam) * predicted.eq(targets_b).sum().item())
        else:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            _, predicted = outputs.max(1)
            correct += predicted.eq(targets).sum().item()
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item() * inputs.size(0)
        total += targets.size(0)
        
        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})
    
    return running_loss / total, correct / total


def evaluate(model, loader, criterion, device):
    """Evaluate model and return metrics."""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for inputs, targets in tqdm(loader, desc="Evaluating"):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    epoch_loss = running_loss / total
    epoch_acc = correct / total
    macro_f1 = f1_score(all_targets, all_preds, average='macro')
    weighted_score = 0.8 * epoch_acc + 0.2 * macro_f1
    
    return epoch_loss, epoch_acc, macro_f1, weighted_score, all_preds, all_targets


def train_model(
    model,
    train_loader,
    val_loader,
    num_epochs=30,
    lr=1e-3,
    momentum=0.9,
    weight_decay=1e-3,
    use_mixup=True,
    label_smoothing=0.05,
    class_weights=None,
    device=DEVICE,
    save_path="best_model.pth",
):
    """
    Full training loop following Final-Competition strategy:
    - SGD with momentum
    - MultiStepLR scheduler
    - Optional mixup and label smoothing
    - Class weights for imbalanced data
    """
    model = model.to(device)
    
    # Loss function with optional class weights and label smoothing
    weight_tensor = class_weights.to(device) if class_weights is not None else None
    criterion = nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=label_smoothing)
    
    # SGD optimizer (as used in Final-Competition)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    
    # MultiStepLR scheduler: reduce LR at 1/4, 1/2, 3/4 of training
    milestones = [num_epochs // 4, num_epochs // 2, num_epochs * 3 // 4]
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)
    
    best_score = 0.0
    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_score': []}
    
    print(f"\nTraining on {device}")
    print(f"Train samples: {len(train_loader.dataset)}, Val samples: {len(val_loader.dataset)}")
    print(f"Epochs: {num_epochs}, LR: {lr}, Mixup: {use_mixup}")
    print("=" * 60)
    
    for epoch in range(1, num_epochs + 1):
        print(f"\nEpoch {epoch}/{num_epochs}")
        
        # Train
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, use_mixup=use_mixup
        )
        
        # Validate
        val_loss, val_acc, val_f1, val_score, _, _ = evaluate(model, val_loader, criterion, device)
        
        # Update scheduler
        scheduler.step()
        
        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        history['val_score'].append(val_score)
        
        current_lr = scheduler.get_last_lr()[0]
        print(f"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | "
              f"val_acc={val_acc:.4f} | val_f1={val_f1:.4f} | score={val_score:.4f} | lr={current_lr:.6f}")
        
        # Save best model
        if val_score > best_score:
            best_score = val_score
            torch.save(model.state_dict(), save_path)
            print(f"*** Best model saved (score={best_score:.4f}) ***")
    
    print("\n" + "=" * 60)
    print(f"Training complete! Best score: {best_score:.4f}")
    return history


# =============================================================================
# Complete Training Pipeline
# =============================================================================

# =============================================================================
# Test Dataset (for inference)
# =============================================================================

class UrbanSoundTestDataset(Dataset):
    """
    PyTorch Dataset for test set (no labels).
    Returns (Log-Mel spectrogram tensor, file_name).
    """
    
    def __init__(
        self,
        csv_path: Path = TEST_CSV,
        audio_dir: Path = TEST_AUDIO_DIR,
        audio_config=None,
        cache_dir: Path | None = None,
    ):
        self.csv_path = Path(csv_path)
        self.audio_dir = Path(audio_dir)
        self.df = pd.read_csv(self.csv_path)
        self.audio_config = audio_config or AUDIO_CONFIG
        self.cache_dir = Path(cache_dir) if cache_dir is not None else None
        if self.cache_dir is not None:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

    def __len__(self):
        return len(self.df)

    def _cache_path(self, file_name):
        if self.cache_dir is None:
            return None
        stem = Path(file_name).stem
        return self.cache_dir / f"mel_test_{stem}.npy"

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_name = row["slice_file_name"]

        cache_path = self._cache_path(file_name)
        if cache_path is not None and cache_path.exists():
            mel = np.load(cache_path)
        else:
            audio_path = self.audio_dir / file_name
            if not audio_path.exists():
                raise FileNotFoundError(f"Audio file not found: {audio_path}")

            mel, _, _ = compute_normalized_mel_spectrogram(
                audio_path,
                config=self.audio_config,
                normalize=True,
            )

            if cache_path is not None:
                np.save(cache_path, mel)

        mel_tensor = torch.from_numpy(mel).unsqueeze(0)  # [1, 128, T]
        return mel_tensor, file_name


def create_test_dataloader(batch_size=32, num_workers=0, cache_dir: Path | None = None):
    """Create DataLoader for test set inference."""
    test_dataset = UrbanSoundTestDataset(
        csv_path=TEST_CSV,
        audio_dir=TEST_AUDIO_DIR,
        audio_config=AUDIO_CONFIG,
        cache_dir=cache_dir,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
    )
    return test_loader


# =============================================================================
# Inference & Submission
# =============================================================================

def predict(model, test_loader, device=DEVICE):
    """
    Generate predictions for test set.
    Returns list of (file_name, predicted_class_id).
    """
    model = model.to(device)
    model.eval()
    
    predictions = []
    
    with torch.no_grad():
        for inputs, file_names in tqdm(test_loader, desc="Predicting"):
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            
            for fname, pred in zip(file_names, predicted.cpu().numpy()):
                predictions.append((fname, int(pred)))
    
    return predictions


def generate_submission(predictions, output_path="submission.csv"):
    """
    Generate Kaggle submission CSV file.
    
    Format:
        ID,TARGET
        0,4
        1,5
        ...
    """
    submission_df = pd.DataFrame({
        "ID": range(len(predictions)),
        "TARGET": [pred for _, pred in predictions]
    })
    
    submission_df["ID"] = submission_df["ID"].astype(int)
    submission_df["TARGET"] = submission_df["TARGET"].astype(int)
    
    submission_df.to_csv(output_path, index=False)
    
    print(f"\nSubmission saved to: {output_path}")
    print(f"Total predictions: {len(submission_df)}")
    print(f"\nFirst 10 rows:")
    print(submission_df.head(10).to_string(index=False))
    print(f"\nClass distribution in predictions:")
    print(submission_df["TARGET"].value_counts().sort_index())
    
    return submission_df


# =============================================================================
# Complete Pipelines
# =============================================================================

def run_training(
    train_folds=(1, 2, 3, 4, 5, 6, 7),
    val_folds=(8,),
    batch_size=32,
    num_epochs=30,
    lr=1e-3,
    num_workers=4,
):
    """Complete training pipeline."""
    print("=" * 60)
    print("Urban Sound Classification - Training")
    print("=" * 60)
    
    cache_dir = BASE_DIR / "mel_cache"
    
    # Create data loaders
    train_loader, val_loader = create_train_val_dataloaders(
        train_folds=train_folds,
        val_folds=val_folds,
        batch_size=batch_size,
        num_workers=num_workers,
        cache_dir=cache_dir,
    )
    
    # Create model
    model = AudioResNet(in_channels=1, num_classes=NUM_CLASSES)
    print(f"\nModel: AudioResNet")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # Compute class weights
    class_weights = compute_class_weights(train_folds)
    print(f"Class weights: {class_weights.numpy().round(2)}")
    
    # Train
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=num_epochs,
        lr=lr,
        use_mixup=True,
        label_smoothing=0.05,
        class_weights=class_weights,
        device=DEVICE,
        save_path="best_model.pth",
    )
    
    return model, history


def run_inference(model_path="best_model.pth", output_path="submission.csv", 
                  batch_size=64, num_workers=4):
    """Complete inference pipeline."""
    print("=" * 60)
    print("Urban Sound Classification - Inference")
    print("=" * 60)
    
    cache_dir = BASE_DIR / "mel_cache"
    
    # Load model
    print("\nLoading model...")
    model = AudioResNet(in_channels=1, num_classes=NUM_CLASSES)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model = model.to(DEVICE)
    print(f"Model loaded from: {model_path}")
    
    # Create test loader
    print("\nCreating test data loader...")
    test_loader = create_test_dataloader(
        batch_size=batch_size,
        num_workers=num_workers,
        cache_dir=cache_dir,
    )
    print(f"Test samples: {len(test_loader.dataset)}")
    
    # Generate predictions
    print("\nGenerating predictions...")
    predictions = predict(model, test_loader, device=DEVICE)
    
    # Save submission
    df = generate_submission(predictions, output_path=output_path)
    
    return df


def run_full_pipeline(
    train_folds=(1, 2, 3, 4, 5, 6, 7),
    val_folds=(8,),
    batch_size=32,
    num_epochs=30,
    lr=1e-3,
    num_workers=4,
):
    """
    Run complete pipeline: Training + Inference.
    
    Example:
        python origin.py  # Will run this by default
    """
    # Check if data exists
    if not TRAIN_CSV.exists():
        print(f"Error: Training CSV not found at {TRAIN_CSV}")
        return None, None
    
    # Training
    model, history = run_training(
        train_folds=train_folds,
        val_folds=val_folds,
        batch_size=batch_size,
        num_epochs=num_epochs,
        lr=lr,
        num_workers=num_workers,
    )
    
    # Inference (if test data exists)
    if TEST_CSV.exists():
        df = run_inference(
            model_path="best_model.pth",
            output_path="submission.csv",
            batch_size=batch_size * 2,
            num_workers=num_workers,
        )
    else:
        print(f"\nWarning: Test CSV not found at {TEST_CSV}")
        print("Skipping inference step.")
        df = None
    
    return model, df


def run_cv_ensemble(
    n_folds=8,
    batch_size=32,
    num_epochs=30,
    lr=1e-3,
    num_workers=4,
    output_path="submission.csv",
):
    """
    8-fold cross-validation with ensemble prediction.
    Trains 8 models and averages their predictions.
    """
    print("=" * 60)
    print("Urban Sound Classification - Cross-Validation Ensemble")
    print("=" * 60)
    
    if not TRAIN_CSV.exists():
        print(f"Error: Training CSV not found at {TRAIN_CSV}")
        return
    
    cache_dir = BASE_DIR / "mel_cache"
    model_paths = []
    val_scores = []
    
    # Train 8 models
    for val_fold in range(1, n_folds + 1):
        print(f"\n{'='*60}")
        print(f"Training Fold {val_fold}/{n_folds}")
        print("=" * 60)
        
        train_folds, val_folds = get_cv_folds(n_folds=n_folds, val_fold=val_fold)
        class_weights = compute_class_weights(train_folds)
        
        train_loader, val_loader = create_train_val_dataloaders(
            train_folds=train_folds,
            val_folds=val_folds,
            batch_size=batch_size,
            num_workers=num_workers,
            cache_dir=cache_dir,
        )
        
        model = AudioResNet(in_channels=1, num_classes=NUM_CLASSES)
        save_path = f"model_fold{val_fold}.pth"
        
        history = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=num_epochs,
            lr=lr,
            use_mixup=True,
            label_smoothing=0.05,
            class_weights=class_weights,
            device=DEVICE,
            save_path=save_path,
        )
        
        model_paths.append(save_path)
        val_scores.append(max(history['val_score']))
        print(f"Fold {val_fold} best score: {max(history['val_score']):.4f}")
    
    # Print CV results
    print("\n" + "=" * 60)
    print("Cross-Validation Results")
    print("=" * 60)
    print(f"Mean CV Score: {np.mean(val_scores):.4f} ± {np.std(val_scores):.4f}")
    print(f"Individual scores: {[f'{s:.4f}' for s in val_scores]}")
    
    # Ensemble inference
    if TEST_CSV.exists():
        print("\n" + "=" * 60)
        print("Ensemble Inference")
        print("=" * 60)
        
        test_loader = create_test_dataloader(
            batch_size=batch_size * 2,
            num_workers=num_workers,
            cache_dir=cache_dir,
        )
        
        # Collect predictions from all models
        all_probs = []
        file_names = None
        
        for i, model_path in enumerate(model_paths):
            print(f"Loading model {i+1}/{len(model_paths)}...")
            model = AudioResNet(in_channels=1, num_classes=NUM_CLASSES)
            model.load_state_dict(torch.load(model_path, map_location=DEVICE))
            model = model.to(DEVICE)
            model.eval()
            
            probs = []
            names = []
            with torch.no_grad():
                for inputs, fnames in tqdm(test_loader, desc=f"Model {i+1}"):
                    inputs = inputs.to(DEVICE)
                    outputs = model(inputs)
                    prob = F.softmax(outputs, dim=1)
                    probs.append(prob.cpu().numpy())
                    names.extend(fnames)
            
            all_probs.append(np.vstack(probs))
            if file_names is None:
                file_names = names
        
        # Average probabilities and get final predictions
        avg_probs = np.mean(all_probs, axis=0)
        predictions = [(name, int(np.argmax(avg_probs[i]))) 
                      for i, name in enumerate(file_names)]
        
        # Generate submission
        df = generate_submission(predictions, output_path=output_path)
        print(f"\nEnsemble submission saved to: {output_path}")
    else:
        print(f"\nWarning: Test CSV not found at {TEST_CSV}")
    
    return val_scores


def main():
    """
    Main entry point with command line argument support.
    
    Usage:
        python origin.py                    # Sanity check only
        python origin.py --train            # Single training + inference
        python origin.py --cv               # 8-fold CV ensemble (best results)
        python origin.py --infer model.pth  # Inference only
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="Urban Sound Classification")
    parser.add_argument("--train", action="store_true", help="Run single training + inference")
    parser.add_argument("--cv", action="store_true", help="Run 8-fold CV ensemble")
    parser.add_argument("--infer", type=str, default=None, help="Run inference with specified model")
    parser.add_argument("--epochs", type=int, default=30, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--workers", type=int, default=4, help="Number of workers")
    
    args = parser.parse_args()
    
    # Print configuration
    print("=" * 60)
    print("Urban Sound Classification")
    print("=" * 60)
    print(f"Device: {DEVICE}")
    print(f"BASE_DIR: {BASE_DIR}")
    print(f"TRAIN_CSV exists: {TRAIN_CSV.exists()}")
    print(f"TEST_CSV exists: {TEST_CSV.exists()}")
    print(f"AUDIO_DIR exists: {AUDIO_DIR.exists()}")
    
    if args.cv:
        # 8-fold Cross-Validation Ensemble (Best results)
        print("\nMode: 8-fold Cross-Validation Ensemble")
        run_cv_ensemble(
            n_folds=8,
            batch_size=args.batch_size,
            num_epochs=args.epochs,
            lr=args.lr,
            num_workers=args.workers,
            output_path="submission.csv",
        )
    
    elif args.train:
        # Single training + inference
        print("\nMode: Single Training + Inference")
        run_full_pipeline(
            train_folds=(1, 2, 3, 4, 5, 6, 7),
            val_folds=(8,),
            batch_size=args.batch_size,
            num_epochs=args.epochs,
            lr=args.lr,
            num_workers=args.workers,
        )
    
    elif args.infer:
        # Inference only
        print(f"\nMode: Inference with model {args.infer}")
        run_inference(
            model_path=args.infer,
            output_path="submission.csv",
            batch_size=args.batch_size * 2,
            num_workers=args.workers,
        )
    
    else:
        # Sanity check (default)
        print("\nMode: Sanity Check")
        print("Use --train, --cv, or --infer to run actual training/inference\n")
        
        if TRAIN_CSV.exists() and AUDIO_DIR.exists():
            # Show data distribution
            show_class_distribution()
            
            # Quick DataLoader check
            try:
                cache_dir = BASE_DIR / "mel_cache"
                train_loader, val_loader = create_train_val_dataloaders(
                    batch_size=4,
                    num_workers=0,
                    cache_dir=cache_dir,
                )
                
                batch = next(iter(train_loader))
                inputs, targets = batch
                print(f"\nDataLoader check passed:")
                print(f"  inputs shape: {inputs.shape}  # [B, 1, n_mels, time]")
                print(f"  targets shape: {targets.shape}")
            except Exception as e:
                print(f"\n[Warning] DataLoader check failed: {e}")


if __name__ == "__main__":
    main()
