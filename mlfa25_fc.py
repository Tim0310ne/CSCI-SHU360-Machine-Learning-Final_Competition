# -*- coding: utf-8 -*-
"""MLFA25-FC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HRO8v6gv0n8MHOXCGtmkLqv5-MpsuAh
"""

"""
One piece of audio = One piece of image
"""

import os
from pathlib import Path

import librosa
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset

# Data paths
# You can either:
# - set the environment variable MLFA25_BASE_DIR to your dataset root, or
# - directly replace "Your Path" with the absolute path to the dataset root.
BASE_DIR = Path(os.getenv("MLFA25_BASE_DIR", "Your Path"))
TRAIN_CSV = BASE_DIR / "metadata" / "kaggle_train.csv"
AUDIO_DIR = BASE_DIR / "audio"

# Default audio / feature configuration for Phase 1
AUDIO_CONFIG = {
    "sr": 22050,
    "n_mels": 128,
    "n_fft": 2048,
    "hop_length": 512,
    "duration": 4.0,  # seconds, None means use full clip
    "fmax": 8000,
}

def audio_to_mel_spectrogram(audio_path, sr=22050, n_mels=128, n_fft=2048,
                             hop_length=512, duration=None, fmax=8000):
    """
    Convert audio file to Mel spectrogram (common CNN input format)

    Args:
        audio_path: Path to audio file
        sr: Sample rate (default 22050 Hz)
        n_mels: Number of Mel filter banks (default 128, corresponds to image height)
        n_fft: FFT window size
        hop_length: Hop length for STFT
        duration: Fixed duration in seconds, if specified will truncate or pad

    Returns:
        mel_spec: Mel spectrogram (n_mels, time_frames)
        y: Audio signal
        sr: Sample rate
    """
    # Load audio
    y, original_sr = librosa.load(str(audio_path), sr=sr)

    # Truncate or pad if fixed duration is specified
    if duration is not None:
        target_length = int(sr * duration)
        if len(y) > target_length:
            y = y[:target_length]
        elif len(y) < target_length:
            y = np.pad(y, (0, target_length - len(y)), mode='constant')

    # Compute Mel spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_mels=n_mels,
        n_fft=n_fft,
        hop_length=hop_length,
        fmax=fmax  # Typically focus on 0-8kHz
    )

    # Convert to log scale (Log-Mel Spectrogram, more commonly used)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    return mel_spec_db, y, sr


def compute_log_mel_spectrogram(audio_path, config=None, normalize=True):
    """
    High-level helper:
    audio file path -> (optionally normalized) Log-Mel spectrogram

    Args:
        audio_path: Path to audio file
        config: dict with keys like AUDIO_CONFIG
        normalize: Whether to scale to [0, 1]

    Returns:
        mel: np.ndarray of shape (n_mels, time_frames), float32
        y: audio signal
        sr: sample rate
    """
    if config is None:
        config = AUDIO_CONFIG

    mel_spec_db, y, sr = audio_to_mel_spectrogram(
        audio_path=audio_path,
        sr=config.get("sr", 22050),
        n_mels=config.get("n_mels", 128),
        n_fft=config.get("n_fft", 2048),
        hop_length=config.get("hop_length", 512),
        duration=config.get("duration", None),
        fmax=config.get("fmax", 8000),
    )

    mel = mel_spec_db
    if normalize:
        mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)

    return mel.astype(np.float32), y, sr


def process_training_sample(row_index=0, save_npy=True):
    """
    Process a training sample and convert to CNN input format

    Args:
        row_index: Data index
        save_npy: Whether to save as .npy file

    Returns:
        Dictionary containing mel_spectrogram, shape, class_id, class_name, file_name
    """
    print("=" * 60)
    print("Convert audio to CNN input format (Mel Spectrogram)")
    print("=" * 60)

    # Read training data
    df_train = pd.read_csv(TRAIN_CSV)
    row = df_train.iloc[row_index]

    file_name = row['slice_file_name']
    fold = row['fold']
    class_name = row['class']
    class_id = row['classID']

    print(f"\nProcessing sample:")
    print(f"  File name: {file_name}")
    print(f"  Class: {class_name} (ID: {class_id})")
    print(f"  Fold: {fold}")

    # Build audio path
    audio_path = AUDIO_DIR / f"fold{fold}" / file_name

    if not audio_path.exists():
        print(f"Error: File not found: {audio_path}")
        return None

    # Convert to Mel spectrogram
    print(f"\nConverting to Mel spectrogram...")
    print(f"  Parameters: n_mels=128, sr=22050, hop_length=512")

    mel_spec, y, sr = compute_log_mel_spectrogram(
        audio_path,
        config=AUDIO_CONFIG,
        normalize=True,
    )

    print(f"\n✓ Conversion complete!")
    print(f"  Original audio length: {len(y) / sr:.2f} seconds")
    print(f"  Sample rate: {sr} Hz")
    print(f"  Mel spectrogram shape: {mel_spec.shape}")
    print(f"    - Height (frequency dimension): {mel_spec.shape[0]} (Mel bins)")
    print(f"    - Width (time dimension): {mel_spec.shape[1]} (time frames)")

    print(f"\n  Normalized range: [{mel_spec.min():.3f}, {mel_spec.max():.3f}]")

    # Save as numpy array (can be directly used for training)
    if save_npy:
        npy_path = f"cnn_input_{row_index}.npy"
        np.save(npy_path, mel_spec)
        print(f"✓ Saved as numpy array: {npy_path}")
        print(f"  File size: {Path(npy_path).stat().st_size / 1024:.2f} KB")
        print(f"  Can be loaded with np.load('{npy_path}') for CNN training")

    return {
        'mel_spectrogram': mel_spec,
        'shape': mel_spec.shape,
        'class_id': class_id,
        'class_name': class_name,
        'file_name': file_name
    }


class UrbanSoundDataset(Dataset):
    """
    PyTorch Dataset that reads kaggle_train.csv and returns
    (Log-Mel spectrogram tensor, classID).
    """

    def __init__(
        self,
        csv_path: Path = TRAIN_CSV,
        audio_dir: Path = AUDIO_DIR,
        folds=None,
        audio_config=None,
        cache_dir: Path | None = None,
    ):
        self.csv_path = Path(csv_path)
        self.audio_dir = Path(audio_dir)
        self.df = pd.read_csv(self.csv_path)

        if folds is not None:
            self.df = self.df[self.df["fold"].isin(folds)].reset_index(drop=True)

        self.audio_config = audio_config or AUDIO_CONFIG
        self.cache_dir = Path(cache_dir) if cache_dir is not None else None
        if self.cache_dir is not None:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

    def __len__(self):
        return len(self.df)

    def _cache_path(self, fold, file_name):
        if self.cache_dir is None:
            return None
        stem = Path(file_name).stem
        return self.cache_dir / f"mel_fold{fold}_{stem}.npy"

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_name = row["slice_file_name"]
        fold = row["fold"]
        class_id = int(row["classID"])

        cache_path = self._cache_path(fold, file_name)
        if cache_path is not None and cache_path.exists():
            mel = np.load(cache_path)
        else:
            audio_path = self.audio_dir / f"fold{fold}" / file_name
            if not audio_path.exists():
                raise FileNotFoundError(f"Audio file not found: {audio_path}")

            mel, _, _ = compute_log_mel_spectrogram(
                audio_path,
                config=self.audio_config,
                normalize=True,
            )

            if cache_path is not None:
                np.save(cache_path, mel)

        # (n_mels, time) -> (1, n_mels, time) for CNN input
        mel_tensor = torch.from_numpy(mel).unsqueeze(0)  # [1, 128, T]
        label_tensor = torch.tensor(class_id, dtype=torch.long)
        return mel_tensor, label_tensor


def create_train_val_dataloaders(
    train_folds=(1, 2, 3, 4, 5, 6, 7, 8),
    val_folds=(9,),
    batch_size=32,
    num_workers=0,
    cache_dir: Path | None = None,
):
    """
    Convenience function to build train/val DataLoaders for Phase 1.
    """
    train_dataset = UrbanSoundDataset(
        csv_path=TRAIN_CSV,
        audio_dir=AUDIO_DIR,
        folds=list(train_folds),
        audio_config=AUDIO_CONFIG,
        cache_dir=cache_dir,
    )
    val_dataset = UrbanSoundDataset(
        csv_path=TRAIN_CSV,
        audio_dir=AUDIO_DIR,
        folds=list(val_folds),
        audio_config=AUDIO_CONFIG,
        cache_dir=cache_dir,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
    )

    return train_loader, val_loader


def show_class_distribution(csv_path: Path = TRAIN_CSV):
    """
    Simple EDA: print per-class and per-fold sample counts.
    """
    df = pd.read_csv(csv_path)
    print("\nClass distribution (class -> count):")
    print(df["class"].value_counts())

    print("\nFold distribution (fold -> count):")
    print(df["fold"].value_counts().sort_index())


def plot_random_mel_examples(dataset: Dataset, num_samples: int = 3):
    """
    Plot a few random Log-Mel spectrograms from a Dataset for visual EDA.

    This function is meant to be called interactively, e.g. from a notebook.
    """
    import random

    import matplotlib.pyplot as plt
    import librosa.display

    n_samples = min(num_samples, len(dataset))
    indices = random.sample(range(len(dataset)), n_samples)

    for i, idx in enumerate(indices, start=1):
        mel_tensor, label_tensor = dataset[idx]
        mel = mel_tensor.squeeze(0).numpy()  # (n_mels, time)

        plt.figure(figsize=(8, 4))
        librosa.display.specshow(
            mel,
            sr=AUDIO_CONFIG["sr"],
            hop_length=AUDIO_CONFIG["hop_length"],
            x_axis="time",
            y_axis="mel",
        )
        plt.colorbar(format="%+2.0f")
        plt.title(f"Sample {idx} (classID={int(label_tensor)})")
        plt.tight_layout()

    plt.show()


def main():
    """Main function"""
    # Sanity check: paths
    print("BASE_DIR:", BASE_DIR)
    print("TRAIN_CSV exists:", TRAIN_CSV.exists())
    print("AUDIO_DIR exists:", AUDIO_DIR.exists())

    # Process first training sample
    result = process_training_sample(row_index=0, save_npy=True)

    if result:
        print("\n" + "=" * 60)
        print("Processing Summary")
        print("=" * 60)
        print(f"Output shape: {result['shape']}")
        print(f"Class: {result['class_name']} (ID: {result['class_id']})")
        print(f"File: {result['file_name']}")

    # Quick DataLoader sanity check (only if data is available)
    if TRAIN_CSV.exists() and AUDIO_DIR.exists():
        try:
            print("\n" + "=" * 60)
            print("Building train/val DataLoaders for sanity check")
            print("=" * 60)

            cache_dir = BASE_DIR / "mel_cache"
            train_loader, val_loader = create_train_val_dataloaders(
                batch_size=4,
                num_workers=0,
                cache_dir=cache_dir,
            )

            batch = next(iter(train_loader))
            inputs, targets = batch
            print(f"\nOne training batch:")
            print(f"  inputs shape: {inputs.shape}  # [B, 1, n_mels, time]")
            print(f"  targets shape: {targets.shape}")
        except Exception as e:
            print("\n[Warning] Failed to build DataLoaders for sanity check:")
            print(f"  {e}")


if __name__ == "__main__":
    main()

# Finish your dataloader


# Finish your model class


# Train & Eval


# Submit your csv file to kaggle